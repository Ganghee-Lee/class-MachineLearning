{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MachineLearning_assignment03.ipynb","provenance":[],"private_outputs":true,"mount_file_id":"1YRxlZ4H_ZvZAt6PBdthv4D9RgKP3YdqQ","authorship_tag":"ABX9TyOzn80dXEXqNpIDqLxjpFyl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"1YdpWFzMT7Ud","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","path = \"drive/My Drive/Classroom/Machine Learning (2) 2020-1/class-MachineLearning/assignment03/data.csv\"\n","data = np.genfromtxt(path, delimiter=',')\n","\n","x_data = data[:, 0]\n","y_data = data[:, 1]\n","\n","#plt.figure(figsize=(8, 8))\n","plt.scatter(x_data, y_data, color='black')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MpCqKEPtcXhi","colab_type":"code","colab":{}},"source":["#Config\n","learning_rate=0.001\n","m=len(x_data)\n","theta0, theta1=-30, -30\n","\n","# Initialize linear model\n","h=theta1*x_data+theta0\n","\n","# Visualize data and linear model\n","plt.scatter(x_data, y_data, color='black')\n","plt.plot(x_data, h, color='blue')\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.title('Input data')\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zrtK9t6eeO8I","colab_type":"code","colab":{}},"source":["'''\n","3. Objective function\n"," - Define objective function\n","\n","4. Gradient Descent\n"," - Define derivations\n"," - Update theta0 and theta1\n","'''\n","\n","# Define objective function\n","def objective_function() :\n","    J = np.sum((h-y_data)**2) / (2*m)\n","    return J\n","\n","# Define derivations\n","def dev() :    \n","    dev0=np.sum(h-y_data) / m\n","    dev1=np.sum((h-y_data)*x_data) / m\n","    return dev0, dev1\n","\n","# Update theta0 and theta1\n","def gradient_descent() :\n","    return theta0-(learning_rate*dev0), theta1-(learning_rate*dev1)\n","\n","J_list=[]\n","theta0_list=[]\n","theta1_list=[]\n","# Train\n","count=0\n","while(1) :\n","    count+=1\n","    J=objective_function()\n","    J_list.append(J)\n","    theta0_list.append(theta0)\n","    theta1_list.append(theta1)\n","    dev0, dev1 = dev()\n","    temp0, temp1 = theta0, theta1\n","    theta0, theta1 = gradient_descent()\n","    h=theta0+theta1*x_data\n","    if(temp0==theta0) and (temp1==theta1) :\n","        break\n","print(\"theta0 : \", theta0)\n","print(\"theta1 : \", theta1)\n","print(\"loss : \", J)\n","# Visualize data and linear model\n","plt.scatter(x_data, y_data, color='black')\n","plt.plot(x_data, h, color='red')\n","plt.xlabel('x')\n","plt.ylabel('y')\n","plt.title('Linear regression results')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AiWlQhiZe5I_","colab_type":"code","colab":{}},"source":["import mpl_toolkits.mplot3d.axes3d as p3\n","\n","def J(t0, t1) :\n","    h=x_data*t1 + t0\n","    J = np.sum((h-y_data)**2) / (2*m)\n","    return J\n","grid_theta0=np.linspace(start=-30, stop=30, num=601)\n","grid_theta1=np.linspace(start=-30, stop=30, num=601)\n","grid_theta0, grid_theta1=np.meshgrid(grid_theta0, grid_theta1)      # Construct meshgrid between x and y\n","\n","\n","#fig=plt.figure(figsize=(20, 20))\n","#ax=fig.gca(projection='3d')\n","\n","fig = plt.figure(figsize=(20, 10))\n","ax = plt.axes(projection='3d')\n","\n","ax.set_xlabel(\"theta0\")\n","ax.set_ylabel(\"theta1\")\n","ax.set_zlabel(\"J(theta0, theta1)\")\n","'''\n","def error(X, Y, THETA):\n","    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)\n","Z = np.array([error(xaug, y, theta) \n","               for theta in zip(np.ravel(M), np.ravel(B))])\n","'''\n","grid_J=np.array([J(t0, t1) for (t0, t1) in zip(np.ravel(grid_theta0), np.ravel(grid_theta1))])\n","print(grid_J.shape)\n","print(grid_theta0.shape)\n","print(min(grid_J))\n","grid_J = grid_J.reshape(grid_theta0.shape)\n","print(grid_J[40][55])\n","#print(min(grid_J))\n","#ax.plot_surface(grid_theta0, grid_theta1, grid_J, cmap=plt.cm.hot, alpha=0.6)\n","ax.contourf3D(grid_theta0, grid_theta1, grid_J, 200, cmap=plt.cm.rainbow, alpha=0.1)\n","ax.plot(theta0_list,theta1_list, J_list,color = 'r', marker = '*', alpha = .9)\n","\n","#ax.plot([t for t in theta0_list], [t2 for t2 in theta1_list], cost , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2)\n","ax.view_init(40, 45)\n","plt.show()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uX6vul_BlXOM","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","num_samples = 100\n","def get_loss(a0,a1):\n","    theta0, theta1 = np.array([a0,a1])\n","    \n","    Y_hat = x_data*theta1 + theta0\n","    #print(Y_hat.shape)\n","    return mse(Y_hat,y_data)\n","get_loss = np.vectorize(get_loss)\n","def mse(Y,Y_hat):\n","    return np.mean(np.square(Y - Y_hat),axis=0)\n","\n","#X = np.ones((num_samples,2))\n","#X[:,0] = np.random.uniform(-1.,1.,num_samples)\n","#print(X.shape)\n","#a = np.array([5,7])\n","#Y = X.dot(a) + np.random.rand(num_samples)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2hlXTCNjOB-","colab_type":"code","colab":{}},"source":["a0_range = np.linspace(-10,30,50)\n","a1_range = np.linspace(-10,30,50)\n","xx,yy = np.meshgrid(a0_range,a1_range)\n","\n","loss_mesh = get_loss(xx,yy)\n","\n","fig = plt.figure(figsize=(5,5))\n","ax = plt.axes(projection='3d')\n","ax.set_xlabel('a0')\n","ax.set_ylabel('a1')\n","ax.set_zlabel('Loss')\n","ax.plot_surface(xx, yy, loss_mesh, cmap='viridis', alpha=0.8)\n","\n","ax.view_init(30, 200)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNl1SYOIkcij","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from scipy import stats \n","\n","from sklearn.datasets.samples_generator import make_regression \n","\n","\n","'''\n","x, y = make_regression(n_samples = 100, \n","                       n_features=1, \n","                       n_informative=1, \n","                       noise=20,\n","                       random_state=2017)\n","x = x.flatten()\n","slope, intercept, _,_,_ = stats.linregress(x,y)\n","best_fit = np.vectorize(lambda x: x * slope + intercept)\n","'''\n","'''\n","plt.plot(x,y, 'o', alpha=0.5)\n","grid = np.arange(-3,3,0.1)\n","plt.plot(grid,best_fit(grid), '.')\n","'''\n","'''\n","def gradient_descent(x, y, theta_init, step=0.001, maxsteps=0, precision=0.001, ):\n","    costs = []\n","    m = y.size # number of data points\n","    theta = theta_init\n","    history = [] # to store all thetas\n","    preds = []\n","    counter = 0\n","    oldcost = 0\n","    pred = np.dot(x, theta)\n","    error = pred - y \n","    currentcost = np.sum(error ** 2) / (2 * m)\n","    preds.append(pred)\n","    costs.append(currentcost)\n","    history.append(theta)\n","    counter+=1\n","    while abs(currentcost - oldcost) > precision:\n","        oldcost=currentcost\n","        gradient = x.T.dot(error)/m \n","        theta = theta - step * gradient  # update\n","        history.append(theta)\n","        \n","        pred = np.dot(x, theta)\n","        error = pred - y \n","        currentcost = np.sum(error ** 2) / (2 * m)\n","        costs.append(currentcost)\n","        \n","        if counter % 25 == 0: preds.append(pred)\n","        counter+=1\n","        if maxsteps:\n","            if counter == maxsteps:\n","                break\n","        \n","    return history, costs, preds, counter\n","'''\n","xaug = np.c_[np.ones(x_data.shape[0]), x_data]\n","#theta_i = [-15, 40] + np.random.rand(2)\n","#history, cost, preds, iters = gradient_descent(xaug, y, theta_i, step=0.1)\n","#theta = history[-1]\n","\n","\n","\n","#from mpl_toolkits.mplot3d import Axes3D\n","\n","def error(X, Y, THETA):\n","    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)\n","\n","ms = np.linspace(25 - 200 , 25 + 200, 400)\n","bs = np.linspace(10 - 200 , 10 + 200, 400)\n","#ms=np.linspace(-30, 30, 60)\n","#bs=np.linspace(-30, 30, 60)\n","M, B = np.meshgrid(ms, bs)\n","\n","zs = np.array([error(xaug, y_data, theta) \n","               for theta in zip(np.ravel(M), np.ravel(B))])\n","Z = zs.reshape(M.shape)\n","#Z=np.sqrt(M**2 + B**2)\n","fig = plt.figure(figsize=(20, 10))\n","import mpl_toolkits.mplot3d.axes3d as p3\n","ax = plt.axes(projection='3d')\n","#ax.plot_surface(M, B, Z, rstride=1, cstride=1, cmap=plt.cm.hot, alpha=0.6)\n","ax.contourf3D(M, B, Z, 200, cmap=plt.cm.rainbow)\n","plt.show()\n","'''\n","fig.add_axes(ax)\n","ax.set_xlabel('x1', labelpad=30, fontsize=24, fontweight='bold')\n","ax.set_ylabel('x2', labelpad=30, fontsize=24, fontweight='bold')\n","ax.set_zlabel('f(x1,x2)', labelpad=30, fontsize=24, fontweight='bold')\n","ax.view_init(elev=30., azim=30)\n","#ax.plot([theta[0]], [theta[1]], [cost[-1]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7)\n","#ax.plot([history[0][0]], [history[0][1]], [cost[0]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7)\n","print('-'*50)\n","print(history)\n","ax.plot([t[0] for t in history], [t[1] for t in history], cost , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2)\n","#ax.plot([t[0] for t in history], [t[1] for t in history], 0 , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2)\n","\n","fig.suptitle(\"Minimizando f(x1,x2)\", fontsize=24, fontweight='bold')\n","plt.savefig(\"Minimization_image.png\")\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BqnpqWJSroae","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from scipy import stats \n","\n","from sklearn.datasets.samples_generator import make_regression \n","\n","\n","\n","x, y = make_regression(n_samples = 100, \n","                       n_features=1, \n","                       n_informative=1, \n","                       noise=20,\n","                       random_state=2017)\n","\n","x = x.flatten()\n","slope, intercept, _,_,_ = stats.linregress(x,y)\n","best_fit = np.vectorize(lambda x: x * slope + intercept)\n","'''\n","plt.plot(x,y, 'o', alpha=0.5)\n","grid = np.arange(-3,3,0.1)\n","plt.plot(grid,best_fit(grid), '.')\n","'''\n","def gradient_descent(x, y, theta_init, step=0.001, maxsteps=0, precision=0.001, ):\n","    costs = []\n","    m = y.size # number of data points\n","    theta = theta_init\n","    history = [] # to store all thetas\n","    preds = []\n","    counter = 0\n","    oldcost = 0\n","    pred = np.dot(x, theta)\n","    error = pred - y \n","    currentcost = np.sum(error ** 2) / (2 * m)\n","    preds.append(pred)\n","    costs.append(currentcost)\n","    history.append(theta)\n","    counter+=1\n","    while abs(currentcost - oldcost) > precision:\n","        oldcost=currentcost\n","        gradient = x.T.dot(error)/m \n","        theta = theta - step * gradient  # update\n","        history.append(theta)\n","        \n","        pred = np.dot(x, theta)\n","        error = pred - y \n","        currentcost = np.sum(error ** 2) / (2 * m)\n","        costs.append(currentcost)\n","        \n","        if counter % 25 == 0: preds.append(pred)\n","        counter+=1\n","        if maxsteps:\n","            if counter == maxsteps:\n","                break\n","        \n","    return history, costs, preds, counter\n","xaug = np.c_[np.ones(x.shape[0]), x]\n","theta_i = [-15, 40] + np.random.rand(2)\n","history, cost, preds, iters = gradient_descent(xaug, y, theta_i, step=0.1)\n","theta = history[-1]\n","print(theta)\n","print(\"Gradient Descent: {:.2f}, {:.2f} {:d}\".format(theta[0], theta[1], iters))\n","print(\"Least Squares: {:.2f}, {:.2f}\".format(intercept, slope))\n","\n","\n","#from mpl_toolkits.mplot3d import Axes3D\n","\n","def error(X, Y, THETA):\n","    return np.sum((X.dot(THETA) - Y)**2)/(2*Y.size)\n","\n","ms = np.linspace(-30 , 30, 60)\n","bs = np.linspace(-30 , 30, 60)\n","#ms=np.linspace(-30, 30, 60)\n","#bs=np.linspace(-30, 30, 60)\n","M, B = np.meshgrid(ms, bs)\n","###\n","xaug = np.c_[np.ones(x_data.shape[0]), x_data]\n","###\n","zs = np.array([error(xaug, y_data, theta) \n","               for theta in zip(np.ravel(M), np.ravel(B))])\n","Z = zs.reshape(M.shape)\n","#Z=np.sqrt(M**2 + B**2)\n","fig = plt.figure(figsize=(20, 10))\n","import mpl_toolkits.mplot3d.axes3d as p3\n","ax = plt.axes(projection='3d')\n","#ax.plot_surface(M, B, Z, rstride=1, cstride=1, cmap=plt.cm.hot, alpha=0.6)\n","ax.contourf3D(M, B, Z, 200, cmap=plt.cm.rainbow)\n","plt.show()\n","'''\n","fig.add_axes(ax)\n","ax.set_xlabel('x1', labelpad=30, fontsize=24, fontweight='bold')\n","ax.set_ylabel('x2', labelpad=30, fontsize=24, fontweight='bold')\n","ax.set_zlabel('f(x1,x2)', labelpad=30, fontsize=24, fontweight='bold')\n","ax.view_init(elev=30., azim=30)\n","#ax.plot([theta[0]], [theta[1]], [cost[-1]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7)\n","#ax.plot([history[0][0]], [history[0][1]], [cost[0]] , markerfacecolor='r', markeredgecolor='r', marker='o', markersize=7)\n","print('-'*50)\n","print(history)\n","ax.plot([t[0] for t in history], [t[1] for t in history], cost , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2)\n","#ax.plot([t[0] for t in history], [t[1] for t in history], 0 , markerfacecolor='r', markeredgecolor='r', marker='.', markersize=2)\n","\n","fig.suptitle(\"Minimizando f(x1,x2)\", fontsize=24, fontweight='bold')\n","plt.savefig(\"Minimization_image.png\")\n","'''"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIsx8yvpArs9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}